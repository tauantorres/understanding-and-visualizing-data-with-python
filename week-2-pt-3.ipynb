{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Univariate data analyses - NHANES case study`\n",
    "\n",
    "Here we will demonstrate how to use Python and [Pandas](https://pandas.pydata.org/) to perform some basic analyses with univariate data, using the 2015-2016 wave of the [NHANES](https://www.cdc.gov/nchs/nhanes/index.htm) study to illustrate the techniques\n",
    "\n",
    "The following import statements make the libraries that we will need available.  Note that in a Jupyter notebook, you should generally use the `%matplotlib inline` directive, which would not be used when running a script outside of the Jupyter environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "da = pd.read_csv('data/nhanes-2015-2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Frequency tables`\n",
    "\n",
    "The [value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) method can be used to determine the number of times that each distinct value of a variable occurs in a data set.  In statistical terms, this is the \"frequency distribution\" of the variable.  Below we show the frequency distribution of the [DMDEDUC2](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm#DMDEDUC2) variable, which is a variable that reflects a person's level of educational attainment.  The `value_counts` method produces a table with two columns.  The first column contains all distinct observed values for the variable.  The second column contains the number of times each of these values occurs.  Note that the table returned by `value_counts` is actually a Pandas data frame, so can be further processed using any Pandas methods for working with data frames.\n",
    "\n",
    "The numbers 1, 2, 3, 4, 5, 9 seen below are integer codes for the 6 possible non-missing values of the DMDEDUC2 variable.  The meaning of these codes is given in the NHANES codebook located [here](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm#DMDEDUC2), and will be discussed further below.  This table shows, for example, that 1621 people in the data file have DMDEDUC=4, which indicates that the person has completed some college, but has not graduated with a four-year degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DMDEDUC2\n",
       "4.0    1621\n",
       "5.0    1366\n",
       "3.0    1186\n",
       "1.0     655\n",
       "2.0     643\n",
       "9.0       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.DMDEDUC2.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `value_counts` method excludes missing values.  We confirm this below by adding up the number of observations with a DMDEDUC2 value equal to 1, 2, 3, 4, 5, or 9 (there are 5474 such rows), and comparing this to the total number of rows in the data set, which is 5735. This tells us that there are 5735 - 5474 = 261 missing values for this variable (other variables may have different numbers of missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the dataset: 5735\n",
      "Total number of rows in the dataset: 5474\n",
      "How many rows are missing DMDEDUC2 values: 261\n"
     ]
    }
   ],
   "source": [
    "sum_ = da.DMDEDUC2.value_counts().sum()\n",
    "total_da = da.shape[0]\n",
    "print(\"Total number of rows in the dataset:\", total_da)\n",
    "print(\"Total number of rows in the dataset:\", sum_)\n",
    "print(\"How many rows are missing DMDEDUC2 values:\", total_da - sum_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to obtain this result is to locate all the null (missing) values in the data set using the [isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html) Pandas function, and count the number of such locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMDEDUC2\n",
      "False    5474\n",
      "True      261\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "diff = pd.isnull(da.DMDEDUC2)\n",
    "print(diff.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "diff = pd.isnull(da.DMDEDUC2).sum()\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases it is useful to [replace](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html) integer codes with a text label that reflects the code's meaning.  Below we create a new variable called 'DMDEDUC2x' that is recoded with text labels, then we generate its frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DMDEDUC2x\n",
       "Some college/AA    1621\n",
       "College            1366\n",
       "HS/GED             1186\n",
       "<9                  655\n",
       "9-11                643\n",
       "Don't know            3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da[\"DMDEDUC2x\"] = da.DMDEDUC2.replace(\n",
    "                            {\n",
    "                                1: \"<9\", \n",
    "                                2: \"9-11\", \n",
    "                                3: \"HS/GED\", \n",
    "                                4: \"Some college/AA\", \n",
    "                                5: \"College\", \n",
    "                                7: \"Refused\", \n",
    "                                9: \"Don't know\"\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "da.DMDEDUC2x.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to have a relabeled version of the gender variable, so we will construct that now as well.  We will follow a convention here of appending an 'x' to the end of a categorical variable's name when it has been recoded from numeric to string (text) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "da[\"RIAGENDRx\"] = da.RIAGENDR.replace({1: \"Male\", 2: \"Female\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many purposes it is more relevant to consider the proportion of the sample with each of the possible category values, rather than the number of people in each category.  We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DMDEDUC2x\n",
       "Some college/AA    29.612715\n",
       "College            24.954330\n",
       "HS/GED             21.666058\n",
       "<9                 11.965656\n",
       "9-11               11.746438\n",
       "Don't know          0.054805\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = da.DMDEDUC2x.value_counts()  # Frequency counts for each category\n",
    "porcent = x / x.sum()\n",
    "porcent*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases we will want to treat the missing response category as another category of observed response, rather than ignoring it when creating summaries.  Below we create a new category called \"Missing\", and assign all missing values to it usig [fillna](https://pandas.pydata.org/pandas-docs/stable/missing_data.html#filling-missing-values-fillna).  Then we recalculate the frequency distribution.  We see that 4.6% of the responses are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DMDEDUC2x\n",
       "Some college/AA    28.265039\n",
       "College            23.818657\n",
       "HS/GED             20.680035\n",
       "<9                 11.421099\n",
       "9-11               11.211857\n",
       "Missing             4.551003\n",
       "Don't know          0.052310\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da[\"DMDEDUC2x\"] = da.DMDEDUC2x.fillna(\"Missing\")\n",
    "x = da.DMDEDUC2x.value_counts()\n",
    "porcent = x / x.sum()\n",
    "porcent*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to calculate individual summary statistics from one column of a data set.  This can be done using Pandas methods, or with numpy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.34267560889516\n",
      "81.34267560889516\n",
      "78.2\n",
      "78.2\n",
      "92.7\n",
      "92.7\n"
     ]
    }
   ],
   "source": [
    "x = da.BMXWT.dropna()  # Extract all non-missing values of BMXWT into a variable called 'x'\n",
    "print(x.mean()) # Pandas method\n",
    "print(np.mean(x)) # Numpy function\n",
    "\n",
    "print(x.median())\n",
    "print(np.percentile(x, 50))  # 50th percentile, same as the median\n",
    "print(np.percentile(x, 75))  # 75th percentile\n",
    "print(x.quantile(0.75)) # Pandas method for quantiles, equivalent to 75th percentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(size: int = 15):\n",
    "    print(\"-\"*size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas method: \n",
      "=> Mean: 81.34267560889516\n",
      "=> Standard deviation: 21.764409359003896\n",
      "=> Minimum: 32.4\n",
      "=> Maximum: 198.9\n",
      "=> Median: 78.2\n",
      "=> 75th percentile: 92.7\n",
      "=> 25th percentile: 65.9\n",
      "--------------------------------------------------\n",
      "Numpy functions: \n",
      "=> Mean: 81.34267560889516\n",
      "=> Standard deviation: 21.764409359003896\n",
      "=> Minimum: 32.4\n",
      "=> Maximum: 198.9\n",
      "=> Median: 78.2\n",
      "=> 75th percentile: 92.7\n",
      "=> 25th percentile: 65.9\n"
     ]
    }
   ],
   "source": [
    "x = da.BMXWT.dropna()  # Extract all non-missing values of BMXWT into a variable called 'x'\n",
    "\n",
    "# Pandas method\n",
    "print(\"Pandas method: \")\n",
    "print(f\"=> Mean: {x.mean()}\") \n",
    "print(f\"=> Standard deviation: {x.std()}\") \n",
    "print(f\"=> Minimum: {x.min()}\") \n",
    "print(f\"=> Maximum: {x.max()}\")\n",
    "print(f\"=> Median: {x.median()}\")\n",
    "print(f\"=> 75th percentile: {x.quantile(0.75)}\") \n",
    "print(f\"=> 25th percentile: {x.quantile(0.25)}\")\n",
    "\n",
    "line(50)\n",
    "\n",
    "# Numpy functions\n",
    "print(\"Numpy functions: \")\n",
    "print(f\"=> Mean: {np.mean(x)}\")\n",
    "print(f\"=> Standard deviation: {np.std(x, ddof=1)}\")  # Ddof=1 gives the sample standard deviation\n",
    "print(f\"=> Minimum: {np.min(x)}\")\n",
    "print(f\"=> Maximum: {np.max(x)}\")\n",
    "print(f\"=> Median: {np.percentile(x, 50)}\")  \n",
    "print(f\"=> 75th percentile: {np.percentile(x, 75)}\")  \n",
    "print(f\"=> 25th percentile: {np.percentile(x, 25)}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at frequencies for a systolic blood pressure measurement ([BPXSY1](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BPX_I.htm#BPXSY1)).  \"BPX\" here is the NHANES prefix for blood pressure measurements, \"SY\" stands for \"systolic\" blood pressure (blood pressure at the peak of a heartbeat cycle), and \"1\" indicates that this is the first of three systolic blood presure measurements taken on a subject.\n",
    "\n",
    "A person is generally considered to have pre-hypertension when their systolic blood pressure is between 120 and 139, or their diastolic blood pressure is between 80 and 89.  Considering only the systolic condition, we can calculate the proprotion of the NHANES sample who would be considered to have pre-hypertension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 <= Mean <= 139: 0.3741935483870968\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean((da.BPXSY1 >= 120) & (da.BPXSY2 <= 139))  # \"&\" means \"and\"\n",
    "print(f\"120 <= Mean <= 139: {mean}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the propotion of NHANES subjects who are pre-hypertensive based on diastolic blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 <= Mean <= 89: 0.14803836094158676\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean((da.BPXDI1 >= 80) & (da.BPXDI2 <= 89))\n",
    "print(f\"80 <= Mean <= 89: {mean}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we calculate the proportion of NHANES subjects who are pre-hypertensive based on either systolic or diastolic blood pressure.  Since some people are pre-hypertensive under both criteria, the proportion below is less than the sum of the two proportions calculated above.\n",
    "\n",
    "Since the combined systolic and diastolic condition for pre-hypertension is somewhat complex, below we construct temporary variables 'a' and 'b' that hold the systolic and diastolic pre-hypertensive status separately, then combine them with a \"logical or\" to obtain the final status for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43975588491717527\n"
     ]
    }
   ],
   "source": [
    "a = (da.BPXSY1 >= 120) & (da.BPXSY2 <= 139)\n",
    "b = (da.BPXDI1 >= 80) & (da.BPXDI2 <= 89)\n",
    "print(np.mean(a | b))  # \"|\" means \"or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
